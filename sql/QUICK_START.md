# ðŸš€ Quick Start: Database Setup & RAG Ingestion

**Complete workflow from zero to working proposal writer in 15 minutes.**

---

## Prerequisites

- âœ… Converted case studies to markdown (with YAML frontmatter)
- âœ… Supabase account created
- âœ… OpenAI API key (or Ollama running locally)
- âœ… Environment variables configured in `.env`

---

## Step 1: Database Setup (5 minutes)

### Option A: Fresh Install (New Database)

**In Supabase SQL Editor:**

```sql
-- Just run this one file - it does everything!
-- Copy and paste: sql/schema.sql
```

**What it creates:**
- âœ… PGVector extension
- âœ… `documents` table with embeddings
- âœ… Vector similarity indexes
- âœ… `match_documents()` RPC function

### Option B: Reset Project Tables Only

**If you want to keep other Supabase tables but reset proposal writer data:**

```sql
-- Step 1: Reset project tables only
-- Copy and paste: sql/reset.sql

-- Step 2: Recreate tables
-- Copy and paste: sql/schema.sql
```

### Option C: Complete Database Wipe (Nuclear Option)

**If you want to delete EVERYTHING in Supabase and start from zero:**

```sql
-- Step 1: FULL RESET (âš ï¸âš ï¸âš ï¸ DELETES ALL TABLES)
-- Copy and paste: sql/reset_full.sql

-- Step 2: Recreate tables
-- Copy and paste: sql/schema.sql
```

### Verify Setup

```sql
-- Quick verification (should return 3 rows)
SELECT 'PGVector' as component, COUNT(*)::text as status FROM pg_extension WHERE extname = 'vector'
UNION ALL
SELECT 'documents table', COUNT(*)::text FROM information_schema.tables WHERE table_name = 'documents'
UNION ALL
SELECT 'match_documents', COUNT(*)::text FROM pg_proc WHERE proname = 'match_documents';
```

**Expected output:**
```
component          | status
-------------------|--------
PGVector          | 1
documents table   | 1
match_documents   | 1
```

---

## Step 2: Prepare Your Files (2 minutes)

### Directory Structure

```
Files/
â”œâ”€â”€ Brainforge_AI_Capabilities.md          # AI/ML capability deck
â”œâ”€â”€ Brainforge_Data_Capabilities.md        # Data/Analytics deck
â”œâ”€â”€ ABC_Home_Case_Study.md                 # Case study 1
â”œâ”€â”€ Amazon_Dashboard_Case_Study.md         # Case study 2
â”œâ”€â”€ Healthcare_AI_Case_Study.md            # Case study 3
â””â”€â”€ ... (more case studies)
```

### Check Your Files

**Verify frontmatter format:**

```bash
# On Windows
type Files\ABC_Home_Case_Study.md | findstr /C:"---" /C:"title:" /C:"industry:"

# On Linux/Mac
head -20 Files/ABC_Home_Case_Study.md | grep -E "^(---|title:|industry:|project_type:)"
```

**Should see:**
```yaml
---
title: "Project Name"
client: "Client Name"
industry: "E-commerce"
project_type: "AI_ML"
tech_stack: ["Python", "FastAPI"]
---
```

---

## Step 3: RAG Ingestion (5 minutes)

### Run the Ingestion Pipeline

```bash
# From project root
python RAG_Pipeline/Local_Files/main.py --directory "./Files"
```

### What You'll See

```
Starting RAG ingestion for directory: ./Files
Found 10 markdown files to process...

Processing: Brainforge_AI_Capabilities.md
  âœ“ Extracted YAML frontmatter
  âœ“ Created 8 chunks (400 chars each)
  âœ“ Generated embeddings (1536 dims)
  âœ“ Inserted 8 chunks into Supabase

Processing: ABC_Home_Case_Study.md
  âœ“ Extracted YAML frontmatter
  âœ“ Created 12 chunks
  âœ“ Generated embeddings
  âœ“ Inserted 12 chunks into Supabase

... (continues for all files)

âœ… Ingestion complete!
Total files processed: 10
Total chunks created: 95
Total time: 3m 45s
```

### Verify Ingestion

**In Supabase SQL Editor:**

```sql
-- Count total documents
SELECT COUNT(*) as total_chunks FROM documents;

-- Count by file
SELECT
    metadata->>'file_title' as file,
    COUNT(*) as chunks
FROM documents
GROUP BY metadata->>'file_title'
ORDER BY chunks DESC;

-- Verify embeddings exist
SELECT COUNT(*) as chunks_with_embeddings
FROM documents
WHERE embedding IS NOT NULL;
```

---

## Step 4: Test RAG Search (3 minutes)

### Test Semantic Search

```sql
-- Test 1: Find AI-related projects
SELECT
    metadata->>'file_title' as title,
    metadata->>'project_type' as type,
    similarity
FROM match_documents(
    query_embedding := (
        SELECT embedding
        FROM documents
        WHERE metadata->>'file_title' ILIKE '%AI%'
        LIMIT 1
    ),
    match_count := 5
)
ORDER BY similarity DESC;
```

**Expected:** Returns 5 most similar documents with similarity scores (0.7-0.95)

### Test Metadata Filtering

```sql
-- Test 2: Find all e-commerce projects
SELECT DISTINCT
    metadata->>'file_title' as project,
    metadata->>'industry' as industry,
    metadata->>'project_type' as type
FROM documents
WHERE metadata->>'industry' = 'E-commerce';
```

**Expected:** Returns all e-commerce case studies

### Test Technology Search

```sql
-- Test 3: Find projects using Python
SELECT DISTINCT
    metadata->>'file_title' as project,
    metadata->'tech_stack' as technologies
FROM documents
WHERE metadata->'tech_stack' ? 'Python'
LIMIT 10;
```

**Expected:** Returns projects with Python in tech_stack

---

## Step 5: Test the Proposal Writer (3 minutes)

### Launch Streamlit

```bash
streamlit run streamlit_ui.py
```

### Generate a Test Proposal

**Paste this sample job posting:**

```
We're looking for an AI developer to build a customer support chatbot
for our e-commerce platform. Must have experience with OpenAI, Python,
and FastAPI. Our company sells sustainable home products.

Requirements:
- Build conversational AI agent
- Integrate with existing CRM
- Handle 100+ conversations/day
- Response time <2 seconds

Budget: $8,000-$12,000
```

### What Should Happen

**The agent will:**
1. âœ… Parse job posting (AI, chatbot, e-commerce)
2. âœ… Search for "AI capabilities deck"
3. âœ… Search for AI/chatbot case studies
4. âœ… Retrieve 1 deck + 2-3 case studies
5. âœ… Generate proposal with specific metrics
6. âœ… Score quality (should be â‰¥8/10)

**Output should include:**
- Reference to AI capabilities deck
- 2-3 specific case study examples
- Metrics like "85% automation" or "90% error reduction"
- Quality score â‰¥8/10

---

## ðŸŽ¯ Success Checklist

After completing all steps, verify:

### Database
- [ ] PGVector extension enabled
- [ ] `documents` table exists with data
- [ ] Embeddings column populated (all rows have vectors)
- [ ] Metadata contains frontmatter fields
- [ ] `match_documents()` function works

### Files
- [ ] 10-15+ case studies in `Files/` directory
- [ ] All files have valid YAML frontmatter
- [ ] At least 1 AI deck + 1 Data deck
- [ ] Files follow naming convention

### RAG Pipeline
- [ ] Ingestion completed without errors
- [ ] Total chunks â‰¥50 (from 10-15 files)
- [ ] Semantic search returns relevant results
- [ ] Metadata filtering works

### Proposal Writer
- [ ] Streamlit UI loads without errors
- [ ] Can generate proposal for AI job
- [ ] Can generate proposal for Data job
- [ ] Quality scores â‰¥8/10
- [ ] References specific case studies

---

## ðŸ› Troubleshooting

### Issue: "PGVector extension not found"

**Solution:**
```sql
-- In Supabase SQL Editor
CREATE EXTENSION IF NOT EXISTS vector;
```

### Issue: "No results from RAG search"

**Verify data exists:**
```sql
SELECT COUNT(*) FROM documents WHERE embedding IS NOT NULL;
```

**If 0 rows:** Re-run ingestion pipeline

### Issue: "Quality scores always <8"

**Possible causes:**
- Not enough case studies (need 10+ minimum)
- Missing metrics in frontmatter
- Brave API not configured (company research disabled)

**Fix:**
1. Add more case studies with metrics
2. Check `.env` has `BRAVE_API_KEY`
3. Verify frontmatter has `key_metrics` field

### Issue: "Embeddings dimension mismatch"

**Error:** `vector has 768 dimensions but expected 1536`

**Fix:** You're using Ollama but schema expects OpenAI dimensions

**Reset and fix:**
```sql
-- Run reset.sql first
-- Then edit schema.sql line 21 and 55:
-- Change VECTOR(1536) to VECTOR(768)
-- Then run schema.sql
```

---

## ðŸ“š Next Steps

After successful setup:

1. **Add more case studies** - Aim for 50+ for best results
2. **Test different job types** - AI vs Data/BI proposals
3. **Review quality scores** - Iterate on frontmatter if scores low
4. **Deploy to production** - See `DEPLOYMENT.md`
5. **Set up monitoring** - Track usage and quality over time

---

## ðŸ†˜ Need Help?

**Documentation:**
- `sql/README.md` - SQL setup details
- `DATA_PREPARATION.md` - Case study formatting
- `USER_GUIDE.md` - Using the proposal writer
- `DEPLOYMENT.md` - Production deployment

**Common Issues:**
- Check `sql/test_queries.sql` for debugging queries
- Review logs: `python RAG_Pipeline/Local_Files/main.py` output
- Verify `.env` file has all required variables

---

## ðŸŽ‰ You're Done!

Your proposal writer is now fully operational! Start generating high-quality, personalized proposals in under 5 minutes.

**Time to first proposal:** ~15 minutes
**Quality score target:** â‰¥8/10
**Case studies needed:** 10-15 minimum, 50+ recommended
